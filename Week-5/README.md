# MySQL Data Integration & Export Pipeline with Airflow

This project demonstrates how to build a Python-based data pipeline that extracts data from a source MySQL database, exports it to multiple file formats (CSV, Parquet, and Avro), and optionally loads it into a destination MySQL database. The pipeline can be automated using Apache Airflow or scheduled manually via cron or Task Scheduler.

---

## ğŸš€ Features

- ğŸ”Œ Connects to MySQL databases using SQLAlchemy + PyMySQL
- ğŸ“¤ Exports data to:
  - CSV (easy to read)
  - Parquet (columnar, efficient for big data)
  - Avro (binary + schema)
- ğŸ” Copies data from source DB to destination DB (table-by-table)
- ğŸ“… Optional: Schedule automation using Airflow, cron, or Windows Task Scheduler
- ğŸ§ª Structured for easy testing and modularization

---

## ğŸ“ Project Structure

```
mysql_data_pipeline/
â”‚
â”œâ”€â”€ main.py                  # Main pipeline runner
â”œâ”€â”€ config.py                # Reads MySQL config from .env
â”œâ”€â”€ requirements.txt         # All dependencies
â”œâ”€â”€ .env                     # Environment variables (DB credentials)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ db.py                # DB connection and fetch logic
â”‚   â””â”€â”€ export.py            # Export logic (CSV, Parquet, Avro)
â”‚
â”œâ”€â”€ output/                  # Exported files go here
â”‚
â””â”€â”€ dags/
    â””â”€â”€ daily_pipeline.py    # Airflow DAG to schedule the pipeline
```

---

## ğŸ”§ Requirements

- Python 3.7 or 3.8
- MySQL server running locally or remotely
- Virtualenv or Conda (recommended)
- [Apache Airflow (optional)](https://airflow.apache.org/docs/)

---

## âœ… Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/your-username/mysql-data-pipeline.git
cd mysql-data-pipeline
```

### 2. Create and activate a virtual environment

```bash
python -m venv venv
.env\Scriptsctivate    # On Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ” .env File Example

Create a file named `.env` and add:

```
SOURCE_USER=root
SOURCE_PASSWORD=your_password
SOURCE_HOST=localhost
SOURCE_PORT=3306
SOURCE_DATABASE=source_db

DEST_USER=root
DEST_PASSWORD=your_password
DEST_HOST=localhost
DEST_PORT=3306
DEST_DATABASE=dest_db
```

---

## â–¶ï¸ Run the Pipeline Manually

```bash
python main.py
```

This will:
- Export each table to `/output/*.csv`, `.parquet`, `.avro`
- Copy the same data to the destination database

---

## ğŸ›  Automate with Airflow 

### 1. Set environment variable before running Airflow

```powershell
$env:AIRFLOW_HOME = "F:/your/path/to/airflow_home"
$env:AIRFLOW__DATABASE__SQL_ALCHEMY_CONN = "sqlite:///F:/your/path/to/airflow_home/airflow.db"
```

### 2. Initialize Airflow DB

```bash
airflow db init
```

### 3. Create Airflow user

```bash
airflow users create --username admin --password admin123 --firstname Admin --lastname User --role Admin --email admin@example.com
```

### 4. Start services

```bash
airflow scheduler
airflow webserver --port 8080
```

Visit [http://localhost:8080](http://localhost:8080)

---

## â° Automate with Task Scheduler (Windows)

1. Open **Task Scheduler**
2. Create Basic Task â†’ Trigger: Daily â†’ Action: Start a Program
3. Program: `python`
4. Arguments: `main.py`
5. Start in: Full path to the project folder

---

## ğŸ“¬ Contact

Maintainer: Piyush  
Feel free to submit issues or pull requests!

